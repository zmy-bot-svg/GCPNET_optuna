# ç›‘æ§è„šæœ¬ - è¯·ä»artifactsä¸­å¤åˆ¶å®Œæ•´å†…å®¹
#!/usr/bin/env python3
"""
GCPNet å®æ—¶è®­ç»ƒç›‘æ§è„šæœ¬
ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨è®­ç»ƒå¼€å§‹å‰è¿è¡Œï¼špython monitor.py &
2. æŸ¥çœ‹å®æ—¶æ—¥å¿—ï¼štail -f monitor.log
3. åœæ­¢ç›‘æ§ï¼škill %1 æˆ–è€…æ‰¾åˆ°PIDå kill PID
"""

import os
import time
import json
import sqlite3
import psutil
import torch
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
import threading
import signal
import sys
import argparse

class TrainingMonitor:
    def __init__(self, check_interval=30, log_file="monitor.log"):
        self.check_interval = check_interval
        self.log_file = log_file
        self.running = True
        
        # è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®åº“æ–‡ä»¶
        self.db_path = self._find_database()
        
        # åˆå§‹åŒ–ç›‘æ§æ•°æ®
        self.last_completed_trial = -1
        self.gpu_memory_history = []
        self.stuck_trial_threshold = 600  # 10åˆ†é’Ÿæ²¡æœ‰è¿›å±•è®¤ä¸ºå¡ä½
        
        print(f"ğŸš€ è®­ç»ƒç›‘æ§å™¨å¯åŠ¨")
        print(f"ğŸ“Š ç›‘æ§æ•°æ®åº“: {self.db_path if self.db_path else 'æœªæ‰¾åˆ°'}")
        print(f"â±ï¸  æ£€æŸ¥é—´éš”: {self.check_interval}ç§’")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        print("="*60)

    def _find_database(self):
        """è‡ªåŠ¨æŸ¥æ‰¾Optunaæ•°æ®åº“æ–‡ä»¶"""
        # åœ¨å½“å‰ç›®å½•åŠå­ç›®å½•ä¸‹æŸ¥æ‰¾.dbæ–‡ä»¶
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.endswith(".db") and ("optuna" in file.lower() or "hyperopt" in file.lower()):
                    found_path = os.path.join(root, file)
                    print(f"ğŸ” è‡ªåŠ¨å‘ç°æ•°æ®åº“: {found_path}")
                    return found_path
        
        print("âš ï¸  æœªæ‰¾åˆ°Optunaæ•°æ®åº“æ–‡ä»¶")
        return None

    def log(self, message, level="INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}][{level}] {message}"
        print(log_message)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_message + "\n")

    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return None
        
        try:
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            allocated = torch.cuda.memory_allocated() / 1e9
            cached = torch.cuda.memory_reserved() / 1e9
            free = total - cached
            
            return {
                "total": total,
                "allocated": allocated,
                "cached": cached,
                "free": free,
                "utilization": (allocated / total) * 100 if total > 0 else 0
            }
        except Exception as e:
            self.log(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}", "ERROR")
            return None

    def get_optuna_status(self):
        """è·å–Optunaè¯•éªŒçŠ¶æ€"""
        if not self.db_path or not os.path.exists(self.db_path):
            return None
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–è¯•éªŒç»Ÿè®¡
            cursor.execute("""
                SELECT state, COUNT(*) 
                FROM trials 
                GROUP BY state
            """)
            state_counts = dict(cursor.fetchall())
            
            # è·å–æœ€æ–°çš„è¯•éªŒä¿¡æ¯
            cursor.execute("""
                SELECT trial_id, state, datetime_start, datetime_complete, value
                FROM trials 
                ORDER BY trial_id DESC 
                LIMIT 5
            """)
            recent_trials = cursor.fetchall()
            
            # è·å–æœ€ä½³è¯•éªŒ
            cursor.execute("""
                SELECT trial_id, value, datetime_complete
                FROM trials 
                WHERE state = 'COMPLETE' AND value IS NOT NULL
                ORDER BY value ASC 
                LIMIT 1
            """)
            best_trial = cursor.fetchone()
            
            conn.close()
            
            return {
                "state_counts": state_counts,
                "recent_trials": recent_trials,
                "best_trial": best_trial
            }
            
        except Exception as e:
            self.log(f"è¯»å–Optunaæ•°æ®åº“å¤±è´¥: {e}", "ERROR")
            return None

    def get_training_processes(self):
        """è·å–è®­ç»ƒç›¸å…³çš„Pythonè¿›ç¨‹"""
        training_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'memory_info', 'cpu_percent', 'create_time']):
            try:
                if proc.info['name'] == 'python' and proc.info['cmdline']:
                    cmdline = ' '.join(proc.info['cmdline'])
                    if any(keyword in cmdline.lower() for keyword in ['main.py', 'optuna', 'hyperparameter']):
                        training_processes.append({
                            'pid': proc.info['pid'],
                            'cmdline': cmdline,
                            'memory_mb': proc.info['memory_info'].rss / 1024 / 1024,
                            'cpu_percent': proc.info['cpu_percent'],
                            'running_time': time.time() - proc.info['create_time']
                        })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        return training_processes

    def detect_stuck_trial(self, optuna_status):
        """æ£€æµ‹å¡ä½çš„è¯•éªŒ"""
        if not optuna_status or not optuna_status['recent_trials']:
            return None
        
        # è·å–æœ€æ–°çš„RUNNINGè¯•éªŒ
        running_trials = [t for t in optuna_status['recent_trials'] if t[1] == 'RUNNING']
        
        if not running_trials:
            return None
        
        latest_running = running_trials[0]
        trial_id = latest_running[0]
        start_time_str = latest_running[2]
        
        if start_time_str:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                start_time = datetime.fromisoformat(start_time_str.replace('Z', ''))
                current_time = datetime.now()
                elapsed = (current_time - start_time).total_seconds()
                
                if elapsed > self.stuck_trial_threshold:
                    return {
                        "trial_id": trial_id,
                        "elapsed_seconds": elapsed,
                        "start_time": start_time_str
                    }
            except Exception as e:
                self.log(f"è§£æè¯•éªŒæ—¶é—´å¤±è´¥: {e}", "WARNING")
        
        return None

    def check_memory_leak(self, gpu_info):
        """æ£€æµ‹GPUå†…å­˜æ³„æ¼"""
        if not gpu_info:
            return False
        
        self.gpu_memory_history.append({
            "timestamp": datetime.now(),
            "allocated": gpu_info["allocated"],
            "cached": gpu_info["cached"]
        })
        
        # åªä¿ç•™æœ€è¿‘30åˆ†é’Ÿçš„æ•°æ®
        cutoff = datetime.now() - timedelta(minutes=30)
        self.gpu_memory_history = [
            h for h in self.gpu_memory_history 
            if h["timestamp"] > cutoff
        ]
        
        # æ£€æµ‹å†…å­˜æ˜¯å¦æŒç»­å¢é•¿
        if len(self.gpu_memory_history) >= 6:  # è‡³å°‘3åˆ†é’Ÿçš„æ•°æ®
            recent = self.gpu_memory_history[-3:]
            older = self.gpu_memory_history[-6:-3]
            
            recent_avg = sum(h["allocated"] for h in recent) / len(recent)
            older_avg = sum(h["allocated"] for h in older) / len(older)
            
            # å¦‚æœå†…å­˜ä½¿ç”¨å¢é•¿è¶…è¿‡0.5GBï¼Œè®¤ä¸ºå¯èƒ½æœ‰æ³„æ¼
            if recent_avg - older_avg > 0.5:
                return True
        
        return False

    def run_check(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´æ£€æŸ¥"""
        # è·å–å„ç§çŠ¶æ€ä¿¡æ¯
        gpu_info = self.get_gpu_memory_info()
        optuna_status = self.get_optuna_status()
        training_processes = self.get_training_processes()
        
        # æ£€æµ‹é—®é¢˜
        stuck_trial = self.detect_stuck_trial(optuna_status)
        memory_leak = self.check_memory_leak(gpu_info)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(gpu_info, optuna_status, training_processes, stuck_trial, memory_leak)
        self.log(report, "REPORT")
        
        # æ£€æŸ¥é‡è¦äº‹ä»¶
        if stuck_trial:
            self.log(f"ğŸš¨ æ£€æµ‹åˆ°å¡ä½çš„è¯•éªŒ: Trial #{stuck_trial['trial_id']} (å·²è¿è¡Œ{stuck_trial['elapsed_seconds']/60:.1f}åˆ†é’Ÿ)", "WARNING")
        
        if memory_leak:
            self.log("ğŸš¨ æ£€æµ‹åˆ°å¯èƒ½çš„GPUå†…å­˜æ³„æ¼", "WARNING")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å®Œæˆçš„trial
        if optuna_status and 'COMPLETE' in optuna_status['state_counts']:
            completed_count = optuna_status['state_counts']['COMPLETE']
            if completed_count > self.last_completed_trial:
                self.log(f"âœ… æ–°å®Œæˆçš„è¯•éªŒï¼Œæ€»è®¡: {completed_count}ä¸ª", "INFO")
                self.last_completed_trial = completed_count

    def generate_report(self, gpu_info, optuna_status, training_processes, stuck_trial, memory_leak):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report_lines = []
        
        # GPUçŠ¶æ€
        if gpu_info:
            report_lines.append(f"ğŸ® GPU - å·²ç”¨:{gpu_info['allocated']:.1f}GB({gpu_info['utilization']:.1f}%) å¯ç”¨:{gpu_info['free']:.1f}GB")
            if gpu_info['free'] < 2.0:
                report_lines.append("âš ï¸  GPUå¯ç”¨å†…å­˜ä¸è¶³2GB")
            if memory_leak:
                report_lines.append("ğŸš¨ æ£€æµ‹åˆ°GPUå†…å­˜æ³„æ¼è¶‹åŠ¿")
        else:
            report_lines.append("âŒ GPUä¸å¯ç”¨")
        
        # OptunaçŠ¶æ€
        if optuna_status:
            state_counts = optuna_status['state_counts']
            total_trials = sum(state_counts.values())
            complete_count = state_counts.get('COMPLETE', 0)
            running_count = state_counts.get('RUNNING', 0)
            failed_count = state_counts.get('FAIL', 0)
            
            report_lines.append(f"ğŸ“Š Trials - æ€»è®¡:{total_trials} å®Œæˆ:{complete_count} è¿è¡Œ:{running_count} å¤±è´¥:{failed_count}")
            
            if optuna_status['best_trial']:
                best = optuna_status['best_trial']
                report_lines.append(f"ğŸ† æœ€ä½³: Trial#{best[0]} MAE:{best[1]:.6f}")
            
            if stuck_trial:
                report_lines.append(f"ğŸš¨ Trial#{stuck_trial['trial_id']} å¯èƒ½å¡ä½({stuck_trial['elapsed_seconds']/60:.1f}åˆ†é’Ÿ)")
        else:
            report_lines.append("âŒ æ— Optunaæ•°æ®")
        
        # è®­ç»ƒè¿›ç¨‹
        if training_processes:
            for proc in training_processes[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªè¿›ç¨‹
                runtime_hours = proc['running_time'] / 3600
                report_lines.append(f"ğŸ”„ PID{proc['pid']} - {proc['memory_mb']:.0f}MB CPU:{proc['cpu_percent']:.1f}% è¿è¡Œ:{runtime_hours:.1f}h")
        else:
            report_lines.append("âŒ æ— è®­ç»ƒè¿›ç¨‹")
        
        return " | ".join(report_lines)

    def monitor_loop(self):
        """ä¸»ç›‘æ§å¾ªç¯"""
        self.log("å¼€å§‹ç›‘æ§å¾ªç¯")
        
        try:
            while self.running:
                self.run_check()
                time.sleep(self.check_interval)
                
        except KeyboardInterrupt:
            self.log("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ç›‘æ§", "INFO")
        except Exception as e:
            self.log(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}", "ERROR")
        finally:
            self.log("ç›‘æ§ç»“æŸ", "INFO")

    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        def signal_handler(signum, frame):
            self.log("æ”¶åˆ°åœæ­¢ä¿¡å·", "INFO")
            self.running = False
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        self.monitor_loop()

def main():
    parser = argparse.ArgumentParser(description="GCPNetè®­ç»ƒå®æ—¶ç›‘æ§")
    parser.add_argument("--interval", type=int, default=30, help="æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’")
    parser.add_argument("--log-file", default="monitor.log", help="æ—¥å¿—æ–‡ä»¶å")
    
    args = parser.parse_args()
    
    monitor = TrainingMonitor(
        check_interval=args.interval,
        log_file=args.log_file
    )
    
    monitor.start()

if __name__ == "__main__":
    main()