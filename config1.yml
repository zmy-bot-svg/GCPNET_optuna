project_name: "GCPNet_base_tuning"      # 建议为新实验起一个明确的名字
net: "GCPNet"
output_dir: "./output_base"
self_loop: True
n_neighbors: 12
debug: False

netAttributes:
  firstUpdateLayers: 4
  secondUpdateLayers: 4
  atom_input_features: 105
  edge_input_features: 50
  triplet_input_features: 40
  embedding_features: 64
  hidden_features: 128                  # <-- 修改点1：增加模型宽度。32维可能太窄，难以学习复杂关系，适当加宽并配合正则化效果更好。
  output_features: 1
  min_edge_distance: 0.0
  max_edge_distance: 8.0
  link: "identity"
  batch_size: 64
  num_workers: 0
  dropout_rate: 0.1                     # <-- 修改点2：启用Dropout。从0.0增加到0.1，这是抑制过拟合最直接的手段。

hyperParameters:
  lr: 0.001
  optimizer: "AdamW"
  optimizer_args:
    weight_decay: 5.0e-5                # <-- 修改点3：增加权重衰减。从1e-5增加到5e-5，提供更强的L2正则化。
  scheduler: "ReduceLROnPlateau"
  scheduler_args:
    mode: "min"
    factor: 0.5                         # <-- 修改点4：增强学习率衰减。从0.8改为0.5，让学习率下降更显著，有助于跳出局部最优。
    patience: 10
    min_lr: 1.0e-6
    threshold: 0.0002
  seed: 666
  epochs: 500
  patience: 30                          # <-- 修改点5：降低早停耐心值。从50降到30，避免在无效训练上浪费过多时间。

data:
  points: all                            # points for the dataset: support "all", and a number smaller than the #. of data points
  dataset_path: './data'                 # path for all datasets
  dataset_name: '2d'                     # name of the dataset, support '2d','cubic', 'mp18','pt', 'mof' etc. 
  # target_index: 2
  target_name: 'property'                # name of the target, support 'property', 'formation_energy_per_atom'
  pin_memory: True                       # whether to pin memory for data loading
  num_folds: 5                           # #. of folds for cross validation
predict:
  model_path: 'model.pt'                 # path for the model
  output_path: 'output.csv'              # path for the predict output

visualize_args:
  perplexity: 50                         # perplexity for t-SNE
  early_exaggeration: 12                 # early_exaggeration for t-SNE
  learning_rate: 300                     # learning_rate for t-SNE
  n_iter: 5000                           # #.of iterations run for t-SNE
  verbose: 1                             # Verbosity level. for t-SNE,support 0,1,2
  random_state: 42                       # random_state for t-SNE

wandb:
  log_enable: True                       # whether to enable wandb, support True, False
  sweep_count: 5                          # integer value to the count parameter to set the maximum number of runs to try.
  entity: "1548532425-null"                    # entity name for wandb,see https://docs.wandb.ai/guides/sweeps/start-sweep-agents
  sweep_args:                            # sweep arguments
    method: random
    parameters:
      lr: 
        distribution: log_uniform_values
        min: 0.000001
        max: 0.1
      
      batch_size:
        distribution: q_uniform
        q: 8
        min: 32
        max: 256

      dropout_rate: 
        distribution: uniform
        min: 0
        max: 0.5

      firstUpdateLayers:
        distribution: q_uniform
        q: 1
        min: 1
        max: 4

      secondUpdateLayers:
        distribution: q_uniform
        q: 1
        min: 1
        max: 4

      hidden_features:
        distribution: q_uniform
        q: 32
        min: 32
        max: 256

# train length: 12000 val length: 1500 test length: 1500 unused length: 0 seed : 666
# <<<<<< ⚡️ cuda is used >>>>>>
# epoch     train_loss  train_mae  train_mape  lr        val_loss  val_mae   val_mape  best_val_mae  time    
# 1.0       0.433       0.433      7.94e+02    0.001     0.334     0.333     2.64e+02  0.334         3.83e+02  
# 2.0       0.258       0.258      2.22e+02    0.001     0.224     0.223     1.53e+02  0.224         3.18e+02  
# 3.0       0.219       0.22       1.78e+02    0.001     0.213     0.213     1.44e+02  0.213         3.47e+02  
# 4.0       0.191       0.191      1.71e+02    0.001     0.186     0.187     1.33e+02  0.186         3.97e+02  
# 5.0       0.185       0.185      98.5        0.001     0.204     0.204     2.09e+02  0.186         3.45e+02  
# 6.0       0.169       0.169      1.91e+02    0.001     0.16      0.16      2.37e+02  0.16          3.13e+02  
# 7.0       0.16        0.16       1.28e+02    0.001     0.166     0.165     1.32e+02  0.16          3.29e+02  
# 8.0       0.151       0.151      1.44e+02    0.001     0.161     0.162     2.29e+02  0.16          3.24e+02  
# 9.0       0.144       0.144      1.42e+02    0.001     0.15      0.149     1.87e+02  0.15          3.39e+02  
# 10.0      0.14        0.14       1.46e+02    0.001     0.19      0.19      2.02e+02  0.15          3.44e+02  
# 11.0      0.14        0.14       1.39e+02    0.001     0.146     0.146     1.31e+02  0.146         3.23e+02  
# 12.0      0.137       0.137      1.11e+02    0.001     0.14      0.14      2.14e+02  0.14          2.91e+02  
# 13.0      0.126       0.126      1.49e+02    0.001     0.133     0.133     1.58e+02  0.133         3e+02     
# 14.0      0.124       0.124      1.39e+02    0.001     0.145     0.145     80.9      0.133         3.29e+02  
# 15.0      0.12        0.12       95.1        0.001     0.122     0.123     85.0      0.122         3.37e+02  
# 16.0      0.12        0.12       93.0        0.001     0.134     0.133     1.62e+02  0.122         2.97e+02  
# 17.0      0.114       0.114      82.7        0.001     0.126     0.126     1.51e+02  0.122         3.19e+02  
# 18.0      0.119       0.12       1.35e+02    0.001     0.131     0.132     98.3      0.122         3.06e+02  
# 19.0      0.109       0.109      1.05e+02    0.001     0.122     0.122     2e+02     0.122         2.79e+02  
# 20.0      0.108       0.108      1.05e+02    0.001     0.124     0.125     1.48e+02  0.122         2.77e+02  
# 21.0      0.106       0.106      96.8        0.001     0.123     0.123     1.56e+02  0.122         2.67e+02  
# 22.0      0.107       0.107      85.4        0.001     0.121     0.121     1.32e+02  0.121         2.76e+02  
# 23.0      0.104       0.104      1.28e+02    0.001     0.118     0.118     94.0      0.118         2.89e+02  
# 24.0      0.102       0.102      1.82e+02    0.001     0.123     0.123     1.12e+02  0.118         2.97e+02  
# 25.0      0.0996      0.0996     1.37e+02    0.001     0.112     0.113     1.52e+02  0.112         2.9e+02   
# 26.0      0.103       0.103      1.14e+02    0.001     0.136     0.136     1.17e+02  0.112         2.85e+02  
# 27.0      0.0979      0.098      87.8        0.001     0.117     0.118     1.58e+02  0.112         2.94e+02  
# 28.0      0.0945      0.0946     1.2e+02     0.001     0.121     0.122     1.3e+02   0.112         2.92e+02  
# 29.0      0.0947      0.0946     69.5        0.001     0.119     0.119     1.34e+02  0.112         2.91e+02  
# 30.0      0.096       0.0961     75.5        0.001     0.127     0.128     1.29e+02  0.112         2.88e+02  
# 31.0      0.0922      0.092      96.3        0.001     0.115     0.115     1.06e+02  0.112         2.85e+02  
# 32.0      0.0903      0.0903     81.5        0.001     0.118     0.118     1.29e+02  0.112         2.67e+02  
# 33.0      0.0895      0.0895     83.8        0.001     0.108     0.108     1.83e+02  0.108         2.71e+02  
# 34.0      0.0889      0.0889     70.3        0.001     0.105     0.106     98.3      0.105         2.53e+02  
# 35.0      0.0845      0.0845     81.5        0.001     0.111     0.112     1.36e+02  0.105         2.61e+02  
# 36.0      0.0868      0.0868     70.5        0.001     0.108     0.109     1.36e+02  0.105         2.86e+02  
# 37.0      0.0825      0.0822     73.3        0.001     0.12      0.121     1.08e+02  0.105         2.85e+02  
# 38.0      0.0835      0.0835     82.2        0.001     0.105     0.106     98.8      0.105         3.19e+02  
# 39.0      0.0815      0.0816     69.7        0.001     0.118     0.119     1.5e+02   0.105         2.9e+02   
# 40.0      0.0827      0.0828     59.8        0.001     0.111     0.111     89.7      0.105         2.61e+02  
# 41.0      0.0789      0.079      98.5        0.001     0.106     0.106     1.38e+02  0.105         2.82e+02  
# 42.0      0.0792      0.0793     78.8        0.001     0.112     0.112     1.22e+02  0.105         2.52e+02  
# 43.0      0.0752      0.0752     74.9        0.001     0.108     0.108     1.02e+02  0.105         2.59e+02  
# 44.0      0.0767      0.0767     61.7        0.001     0.113     0.114     1.48e+02  0.105         2.9e+02   
# 45.0      0.0747      0.0747     83.8        0.001     0.112     0.112     1.31e+02  0.105         2.61e+02  
# 46.0      0.0606      0.0606     74.9        0.0005    0.0942    0.0945    1.13e+02  0.0942        2.67e+02  
# 47.0      0.055       0.055      60.6        0.0005    0.0933    0.0936    1.45e+02  0.0933        2.53e+02  
# 48.0      0.0539      0.0539     52.1        0.0005    0.0922    0.0925    1.32e+02  0.0922        2.8e+02   
# 49.0      0.0544      0.0544     62.4        0.0005    0.0929    0.0929    1.45e+02  0.0922        2.62e+02  
# 50.0      0.0529      0.0529     53.6        0.0005    0.0927    0.0927    1.19e+02  0.0922        2.55e+02  
#     epoch  train_loss  train_mae  train_mape      lr  val_loss   val_mae    val_mape  best_val_mae        time
# 0       1    0.432866   0.433414  793.759094  0.0010  0.334271  0.333467  263.557343      0.334271  383.458754
# 1       2    0.257852   0.257853  221.557663  0.0010  0.223834  0.223253  153.137650      0.223834  318.121897
# 2       3    0.219465   0.219536  177.770813  0.0010  0.212855  0.212937  144.145767      0.212855  347.484287
# 3       4    0.191011   0.191125  170.873184  0.0010  0.186160  0.186662  133.119705      0.186160  397.441182
# 4       5    0.184607   0.184597   98.510925  0.0010  0.204256  0.203721  208.697189      0.186160  344.927714
# 5       6    0.168935   0.168921  191.009933  0.0010  0.159717  0.160121  237.194382      0.159717  313.269202
# 6       7    0.159571   0.159618  128.143265  0.0010  0.165808  0.165211  131.823364      0.159717  328.670781
# 7       8    0.151387   0.151400  144.244110  0.0010  0.161248  0.161883  229.164505      0.159717  324.475143
# 8       9    0.143861   0.143853  142.205017  0.0010  0.149963  0.149338  187.132858      0.149963  339.199238
# 9      10    0.139813   0.139721  145.590347  0.0010  0.189871  0.189916  202.124481      0.149963  344.220221
# 10     11    0.140455   0.140370  139.214859  0.0010  0.146177  0.146013  130.523956      0.146177  322.737290
# 11     12    0.136610   0.136677  110.813164  0.0010  0.139570  0.139567  214.172287      0.139570  290.658329
# 12     13    0.126270   0.126321  148.735229  0.0010  0.133034  0.132686  157.575485      0.133034  300.223459
# 13     14    0.123918   0.123967  139.367081  0.0010  0.145112  0.145337   80.898750      0.133034  328.716910
# 14     15    0.120069   0.120093   95.068069  0.0010  0.122325  0.122773   85.003517      0.122325  336.500289
# 15     16    0.119622   0.119649   92.961769  0.0010  0.133587  0.133204  162.442932      0.122325  296.821331
# 16     17    0.113507   0.113513   82.727524  0.0010  0.125668  0.126401  151.075623      0.122325  319.201532
# 17     18    0.119493   0.119517  134.809311  0.0010  0.130836  0.131574   98.283554      0.122325  305.564238
# 18     19    0.109424   0.109468  104.708565  0.0010  0.121580  0.122336  199.721786      0.121580  278.761738
# 19     20    0.108092   0.108071  104.663239  0.0010  0.124376  0.124796  147.532776      0.121580  276.784549
# 20     21    0.105935   0.105978   96.785072  0.0010  0.122789  0.122732  155.979782      0.121580  266.784812
# 21     22    0.107101   0.106945   85.363060  0.0010  0.121167  0.121232  132.022354      0.121167  276.132032
# 22     23    0.103824   0.103738  127.602226  0.0010  0.117920  0.117750   94.004616      0.117920  289.042762
# 23     24    0.101576   0.101537  182.404129  0.0010  0.123120  0.123467  112.254112      0.117920  297.328518
# 24     25    0.099584   0.099571  136.930557  0.0010  0.112451  0.112555  152.197906      0.112451  289.868774
# 25     26    0.103167   0.103143  113.995941  0.0010  0.135673  0.135962  117.155846      0.112451  285.320398
# 26     27    0.097937   0.097959   87.813820  0.0010  0.117180  0.117524  158.004257      0.112451  293.527257
# 27     28    0.094549   0.094558  120.448410  0.0010  0.121063  0.122085  129.646912      0.112451  292.001931
# 28     29    0.094679   0.094563   69.534050  0.0010  0.119256  0.119376  134.103333      0.112451  290.844925
# 29     30    0.096048   0.096080   75.533142  0.0010  0.127164  0.127551  129.452789      0.112451  287.780321
# 30     31    0.092218   0.092025   96.294617  0.0010  0.115147  0.115241  106.242294      0.112451  285.034882
# 31     32    0.090300   0.090271   81.514496  0.0010  0.118330  0.118393  129.287750      0.112451  267.241457
# 32     33    0.089530   0.089545   83.757889  0.0010  0.107601  0.108249  182.852066      0.107601  271.461310
# 33     34    0.088886   0.088895   70.343613  0.0010  0.105170  0.105501   98.328552      0.105170  253.243524
# 34     35    0.084535   0.084533   81.523689  0.0010  0.111380  0.111786  136.055298      0.105170  260.986263
# 35     36    0.086784   0.086840   70.458366  0.0010  0.108471  0.108868  135.582443      0.105170  286.011238
# 36     37    0.082484   0.082250   73.302521  0.0010  0.120431  0.120673  108.386566      0.105170  284.728998
# 37     38    0.083453   0.083453   82.160881  0.0010  0.105245  0.105691   98.844284      0.105170  319.102218
# 38     39    0.081549   0.081598   69.699638  0.0010  0.117873  0.118540  149.776917      0.105170  289.602053
# 39     40    0.082737   0.082784   59.825249  0.0010  0.110898  0.110992   89.740417      0.105170  260.801755
# 40     41    0.078949   0.078954   98.466843  0.0010  0.105898  0.106063  137.941833      0.105170  281.784513
# 41     42    0.079217   0.079252   78.842102  0.0010  0.111955  0.112380  122.140030      0.105170  252.176531
# 42     43    0.075213   0.075165   74.910721  0.0010  0.107978  0.108377  102.218010      0.105170  259.295587
# 43     44    0.076653   0.076692   61.720535  0.0010  0.113376  0.113962  147.717072      0.105170  289.599077
# 44     45    0.074725   0.074689   83.783096  0.0010  0.111998  0.112492  130.683380      0.105170  260.522127
# 45     46    0.060593   0.060592   74.853142  0.0005  0.094166  0.094477  113.000854      0.094166  266.882817
# 46     47    0.055014   0.054982   60.645134  0.0005  0.093283  0.093568  145.175247      0.093283  253.187983
# 47     48    0.053907   0.053896   52.083176  0.0005  0.092156  0.092527  131.516113      0.092156  279.939608
# 48     49    0.054446   0.054435   62.387379  0.0005  0.092883  0.092922  145.210861      0.092156  261.981244
# 49     50    0.052892   0.052907   53.560307  0.0005  0.092672  0.092682  118.604019      0.092156  255.016815
# C:\Users\15485\OneDrive\Desktop\GCPNet-main\utils\train_utils.py:417: FutureWarning:

# You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.

# 100%|███████████████| 24/24 [00:17<00:00,  1.33it/s, val_loss=0.0883, val_mae=0.0887, val_mape=90.7]
# {'val_loss': 0.08833878285561998, 'val_mae': 0.08872392028570175, 'val_mape': 90.70121002197266}
# 100%|███████████████| 24/24 [00:17<00:00,  1.38it/s, val_loss=0.0883, val_mae=0.0887, val_mape=90.7]
# 100%|███████████████| 24/24 [00:17<00:00,  1.38it/s, val_loss=0.0883, val_mae=0.0887, val_mape=90.7]
# wandb:
# wandb:                                                                                                                       
# wandb: Run history:
# wandb: best_val_loss █▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
# wandb:  best_val_mae █▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
# wandb:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
# wandb:            lr ████████████████████████████████████▁▁▁▁
# wandb:      test_mae ▁
# wandb:     test_mape ▁
# wandb:          time ▇▄▆█▅▅▄▅▅▄▃▅▅▃▄▂▂▂▂▃▃▃▃▃▃▂▁▁▃▃▃▁▂▁▁▁▂▁▂▁
# wandb:  total_params ▁
# wandb:    train_loss █▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁
# wandb:     train_mae █▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
# wandb:    train_mape █▃▂▂▁▂▂▂▂▂▂▁▁▁▂▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
# wandb:      val_loss █▅▄▄▄▃▃▃▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁▂▂▂▁▁▁
# wandb:       val_mae █▅▄▄▄▃▃▃▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▂▁▂▁▁▁▁
# wandb:      val_mape █▄▃▃▆▇▅▆▃▆▁▄▄▂▅▄▃▁▂▄▄▃▃▃▂▅▂▃▃▂▄▁▃▂▂▃▂▃▃▂
# wandb:
# wandb: Run summary:
# wandb:    best_score 0.09216
# wandb: best_val_loss 0.09216
# wandb:  best_val_mae 0.09216
# wandb:         epoch 50
# wandb:            lr 0.0005
# wandb:      test_mae 0.08872
# wandb:     test_mape 90.70119
# wandb:          time 255.01682
# wandb:  total_params 2818017
# wandb:    train_loss 0.05289
# wandb:     train_mae 0.05291
# wandb:    train_mape 53.56031
# wandb:      val_loss 0.09267
# wandb:       val_mae 0.09268
# wandb:      val_mape 118.60402
# wandb:
# wandb:  View run 20250627_185831 at: https://wandb.ai/1548532425-null/GCPNet_base_tuning/runs/2kdikhr0
# wandb:  View project at: https://wandb.ai/1548532425-null/GCPNet_base_tuning
# wandb: Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)
# wandb: Find logs at: .\wandb\run-20250627_185833-2kdikhr0\logs